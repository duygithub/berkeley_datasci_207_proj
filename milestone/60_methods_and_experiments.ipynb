{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc1c7a8",
   "metadata": {},
   "source": [
    "**Target:**\n",
    "\n",
    "Predict expected loan proportions to be paid back, or the expected recovery ratio of the loans.\n",
    "\n",
    "where expected loan recover ratio is the ratio total payment LendingClub (LC) can receive from borrowers to the original total loan amount LC issued.\n",
    "\n",
    "It is calculated as loan_recovery_ratio = total_pymnt / loanAmnt, both variables are present in the data.\n",
    "\n",
    "\n",
    "**Method:** \n",
    "1. Baseline\n",
    "\n",
    "Linear regression with a few features important from EDA process, for example: loanAmnt, interest rate (int_rate), and one‑hot encoded grade/sub_grade.\n",
    "\n",
    "2. Improved model (1)\n",
    "\n",
    "Regularised linear model (Ridge and Lasso) with the full set of engineered features:\n",
    "\n",
    "categorical variables (sub_grade, term, home_ownership, purpose) encoded via one‑hot encoding;\n",
    "\n",
    "borrower characteristics (emp_length, annual_inc, dti, open_acc, revol_util, fico_range_low/high);\n",
    "\n",
    "loan characteristics (installment, issue_d year/month, loan_status indicator).\n",
    "\n",
    "Add polynomial or interaction terms if needed (e.g. int_rate × fico).\n",
    "\n",
    "This improved linear model is expected to capture more explanatory power and improve prediction accuracy.\n",
    "\n",
    "3. Improved model (2)\n",
    "\n",
    "Ensemble methods such as Random Forest. \n",
    "\n",
    "Random Forest handles nonlinear effects well and shows good predictability in many applications.\n",
    "\n",
    "4. Improved model (3)\n",
    "\n",
    "Enhanced tree models such as XGBoost and LightGBM.\n",
    "\n",
    "These methods provide enhanced model capacity in handling nonlinear effects and deliver very good results in many real-world applications.\n",
    "\n",
    "5. Improved model (4)\n",
    "\n",
    "It can be worthwhile to test out a feed-forward neural network if results are still not satisfactory, with the caveat that this model can be overfitting and unnecessarily complex. This model needs hyperparameter tuning and regularization.\n",
    "\n",
    "Gather results and compare. If the model results are still unsatisfactory, we will review the features and perform some feature engineering. Parameter tuning can be another option to boost performance.\n",
    "\n",
    "**Result:**\n",
    "\n",
    "Expected paid-back portion = Predicted Recovery Ratio from our model\n",
    "\n",
    "**Experimental design:**\n",
    "1. Data preparation\n",
    "\n",
    "Remove loans with loan_status “current” or “in grace period”; use only fully charged‑off/fully paid data unless modeling censoring explicitly.\n",
    "Split the dataset chronologically into train (70 %) / validation (15 %) / test (15 %) to mimic origination forecasting and avoid look‑ahead bias. Chronologically split also prevents data leakage. \n",
    "Use k‑fold cross‑validation within training for hyperparameter tuning, currently start with 5 folds.\n",
    "Standardise/scale numeric features for linear models, which is important for linear models to derive appropriate weights. Tree models can be insensitive to scales and no need to particularly scale.\n",
    "\n",
    "2. Hyperparameter tuning\n",
    "\n",
    "* Linear Models (Ridge/Lasso/ Elastic Net):\n",
    "\n",
    "We will perform grid search or random search over the regularization strength (e.g., α).\n",
    "\n",
    "&emsp;If overfitting: we can increase regularization strength (larger α) to reduce model variance.\n",
    "\n",
    "&emsp;If underfitting: we can decrease regularization strength or introduce interaction/polynomial terms to increase model flexibility.\n",
    "\n",
    "\n",
    "\n",
    "* Tree-based models: \n",
    "\n",
    "We will tune learning rate, number of trees, max depth, subsample ratio, colsample_bytree, L1/L2 regularization.\n",
    "\n",
    "&emsp;If overfitting: we can reduce maximum depth, increase regularization, adjust learning rate, use stronger subsampling, and apply early stopping.\n",
    "\n",
    "&emsp;If underfitting: we can increase tree depth, increase number of trees, reduce regularization, adjust learning rate to see if model performance improves.\n",
    "\n",
    "\n",
    "* Neural Network models: \n",
    "\n",
    "We will tune number of layers, activation function, learning rate, batch size, drop out rate, number of epoches\n",
    "\n",
    "&emsp;If overfitting: we can increase dropout rate, add L2 regularization, reduce network size with fewer layers, and apply early stopping\n",
    "\n",
    "&emsp;If underfitting: we can increase number of layers, use more epochs, reduce regularization, and adjust learning rate\n",
    "\n",
    "3. Evaluation Metrics\n",
    "\n",
    "Mean Absolute Error (MAE), Root Mean Squared Error (RMSE): These are classic ML evaluation metrics to compare predicted value with ground truth. Compare these metrics and see which one achieves the least error.\n",
    "\n",
    "R² for linear models to examine the proportion of variance explained. The higher R² is, the better explainability and forecast capacity the model captures.\n",
    "\n",
    "4. Comparison Method\n",
    "\n",
    "Model with the best performance will be chosen. The best model should have the lowest MAE and RMSE. If it is linear model, it should have a high R². Meanwhile, we will compare train and test accuracy to make sure the model generalizes well with no big accuracy gaps. Cross-validation performance will also be considered to assess robustness.\n",
    "   \n",
    "5. Caveat\n",
    "\n",
    "We can only use features at funding to avoid data leakage. Features such as last_pymnt_amnt, recoveries, already hinted recovery and default, that can introduce data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8435da5c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
