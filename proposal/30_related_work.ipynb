{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5c55f5c-7066-4741-b9f2-d4972784859f",
   "metadata": {},
   "source": [
    "Related Work\n",
    "\n",
    "[@Trinh2024]\n",
    "Published in the Journal of Economics, Finance and Administrative Science. This study used a similar peer-to-peer lending data and compared nine different models to evaluate consumer credit risk, which are: Logistic Regression, Naive Bayes, Linear Discriminant Analysis, k-Nearest Neighbor, Support Vector Machine, Classification and Regression Tree, Artificial Neural Network, Random Forest, and Gradient Boosting Decision Tree. It concluded that the Gradient Boosting Decision Tree is the best model for most of the evaluation metrics when predicting default risk.\n",
    "\n",
    "[@Change2015]\n",
    "This work used Lending Club data and fit Logistic Regression, SVM, and Naive Bayes. They concluded that Naive Bayes with Gaussian performs the best at predicting default rate. Furthermore, by applying the best-performing model, they found that the investment return from Lending Club could grow by 50%. \n",
    "\n",
    "[@Souadda2025]\n",
    "This research evaluates Hyperparameter Optimization (HPO) methods among Grid Search, Random Search, Hyperopt, and Optuna, across models, Logistic Regression, Random Forest, XGBoost, and LightGBM, on the Lending Club data. They concluded that LightGBM achieves the highest AUC, with XGBoost performing comparably. Bayesian methods (Hyperopt, Optuna) achieved comparable accuracy to Grid Search while reducing runtime by up to 75.7-fold. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
